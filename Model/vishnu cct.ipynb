{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import glob, random, os, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "import time as t\n",
    "import shutil\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#To get the same results in different environments\n",
    "def seed_everything(seed = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = \"True\"\n",
    "    os.environ[\"TF_DISABLE_SEGMENT_REDUCTION_OP_DETERMINISM_EXCEPTIONS\"] = \"True\"\n",
    "\n",
    "seed_everything()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "print('TensorFlow Version ' + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import layers\n",
    "# from tensorflow import keras\n",
    "# class ConvolutionalTokenizer(layers.Layer):\n",
    "#     \"\"\"\n",
    "#     Creates Convolutional Tokens of images for feeding to Transformer Encoder.\n",
    "#     \"\"\"\n",
    "#     def __init__(self,kernel_size=3,stride=1,padding=1,pooling_kernel_size=3,pooling_stride=2,conv_layers=2,num_output_channels=[64, 128],**kwargs,):\n",
    "#         super(ConvolutionalTokenizer, self).__init__(**kwargs)\n",
    "        \n",
    "#         # Creating a Sequential Keras Model for Tokenizing images\n",
    "#         self.conv_model = keras.Sequential()\n",
    "#         # Created the required number of convolutional layer\n",
    "#         for i in range(conv_layers):\n",
    "#             # Adding a conv2d layer with ReLU activation as suggested by authors\n",
    "#             self.conv_model.add(layers.Conv2D(num_output_channels[i],kernel_size,stride,padding=\"valid\",use_bias=False,activation=\"relu\",kernel_initializer=\"he_normal\"))\n",
    "#             # Zero Padding\n",
    "#             self.conv_model.add(layers.ZeroPadding2D(padding))\n",
    "#             # Pooling over the image with 3x3 kernel having padding='same' and stride=2   \n",
    "#             self.conv_model.add(layers.MaxPool2D(pooling_kernel_size, pooling_stride, \"same\"))\n",
    "\n",
    "#     def call(self, images):\n",
    "#         # Reshaping the outputs by flattening them\n",
    "#         outputs = self.conv_model(images)\n",
    "#         Flattened = tf.reshape(\n",
    "#             outputs,\n",
    "#             (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[3]),\n",
    "#         )\n",
    "#         return Flattened\n",
    "\n",
    "#     # Adding Learnable Positional Embeddings\n",
    "#     def pos_embeddings(self, image_size):\n",
    "#         inp = tf.ones((1, image_size, image_size, 1))\n",
    "#         out = self.call(inp)\n",
    "#         seq_len = tf.shape(out)[1]\n",
    "#         projection_dim = tf.shape(out)[-1]\n",
    "\n",
    "#         embed_layer = layers.Embedding(\n",
    "#             input_dim=seq_len, output_dim=projection_dim\n",
    "#         )\n",
    "#         return embed_layer, seq_len\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "class ConvolutionalTokenizer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Creates Convolutional Tokens of images for feeding to Transformer Encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size=3, stride=1, padding=1, pooling_kernel_size=3, pooling_stride=2,\n",
    "                 conv_layers=2, num_output_channels=[64, 128], **kwargs):\n",
    "        super(ConvolutionalTokenizer, self).__init__(**kwargs)\n",
    "        \n",
    "        # Creating a Sequential Keras Model for Tokenizing images\n",
    "        self.conv_model = keras.Sequential()\n",
    "        # Creating the required number of convolutional layers\n",
    "        for i in range(conv_layers):\n",
    "            # Adding a Conv2D layer with ReLU activation as suggested by authors\n",
    "            self.conv_model.add(layers.Conv2D(num_output_channels[i], kernel_size, stride,\n",
    "                                              padding=\"valid\", use_bias=False, activation=\"relu\",\n",
    "                                              kernel_initializer=\"he_normal\"))\n",
    "            # Zero Padding\n",
    "            self.conv_model.add(layers.ZeroPadding2D(padding))\n",
    "            # Pooling over the image with 3x3 kernel, padding='same' and stride=2   \n",
    "            self.conv_model.add(layers.MaxPool2D(pooling_kernel_size, pooling_stride, \"same\"))\n",
    "\n",
    "    def call(self, images):\n",
    "        # Pass RGB images through the convolutional model\n",
    "        outputs = self.conv_model(images)\n",
    "        # Flatten the output\n",
    "        flattened = tf.reshape(outputs,\n",
    "                               (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[3]))\n",
    "        return flattened\n",
    "\n",
    "    def pos_embeddings(self, image_size):\n",
    "        # Ensure the input has 3 channels for RGB\n",
    "        inp = tf.ones((1, image_size, image_size, 3))  # RGB shape\n",
    "        out = self.call(inp)\n",
    "        seq_len = tf.shape(out)[1]\n",
    "        projection_dim = tf.shape(out)[-1]\n",
    "\n",
    "        # Define the positional embedding layer\n",
    "        embed_layer = layers.Embedding(input_dim=seq_len, output_dim=projection_dim)\n",
    "        return embed_layer, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout):\n",
    "    \"\"\"\n",
    "    Creates A Feed Forward Network`\n",
    "    \n",
    "    Args:\n",
    "        hidden_units: Number of hidden units in MLP\n",
    "        dropout: The Rate of dropout which is to be applied.\n",
    "    \"\"\"\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transformer_Encoder(L,embedded_patches,num_heads,projection_dim,transformer_units):\n",
    "    \"\"\"\n",
    "    Transformer Encoder Block\n",
    "    \n",
    "    Args: \n",
    "        L: number of transformer_layers\n",
    "        \n",
    "        embedded_patches: Patches from the Convolutional Tokenizer block\n",
    "        \n",
    "        num_heads: Number of Attention Heads\n",
    "        \n",
    "        projection_dim: Size of each attention head for query and key\n",
    "        \n",
    "        transformer_units: hidden units of MLP\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Iterating over the number of transformer layers\n",
    "    for i in range(L):\n",
    "        # Normalizing the input patches\n",
    "        norm = layers.LayerNormalization(epsilon=1e-5)(embedded_patches)\n",
    "        # Feeding to MHA\n",
    "        attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(norm,norm)\n",
    "        # Shortcut skip connection\n",
    "        skip1 = layers.Add()([attention_output, embedded_patches])\n",
    "        # Normalizing \n",
    "        norm2= layers.LayerNormalization(epsilon=1e-5)(skip1)\n",
    "        \n",
    "        # Feed Forward MLP\n",
    "        ffn = mlp(norm2, hidden_units=transformer_units, dropout=0.1)\n",
    "\n",
    "        # Shortcut skip connection\n",
    "        embedded_patches = layers.Add()([ffn, skip1])\n",
    "        \n",
    "        return embedded_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SeqPool(trans_enco_out):\n",
    "    \"\"\"\n",
    "    Sequence Pooling block.\n",
    "    \n",
    "    Args: \n",
    "        trans_enco_out: Takes in the Output of transformer encoder block \n",
    "    \n",
    "    Returns:\n",
    "        A 1xD output to be fed to final classifier\n",
    "    \"\"\"\n",
    "    # Normalizing the output of transformer enocder layer\n",
    "    normalized = layers.LayerNormalization(epsilon=1e-5)(trans_enco_out)\n",
    "    # Adding a linear layer\n",
    "    linear=layers.Dense(1)(normalized)\n",
    "    # Applying Softmax to the linear layer\n",
    "    soft = tf.nn.softmax(linear, axis=1)\n",
    "    # Multiplying the softmax of linear layer with the normalized output of orignal output of the transformer encoder block\n",
    "    mult = tf.matmul(soft, normalized, transpose_a=True)\n",
    "    # Squeezing the dimensions\n",
    "    seq_pool_output = tf.squeeze(mult, -2)\n",
    "\n",
    "    return seq_pool_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompactConvolutionalTransformer(image_size=224, num_classes=2, input_shape=(224, 224, 3),\n",
    "                                    projection_dim=128, num_heads=2, L=2, transformer_units=[128, 128]):\n",
    "    \"\"\"\n",
    "    CCT model for RGB input\n",
    "    \n",
    "    Args:\n",
    "        image_size: size of image\n",
    "        num_classes: Number of classes of output\n",
    "        input_shape: shape of image (updated for RGB)\n",
    "        projection_dim: Size of each attention head for query and key\n",
    "        num_heads: Number of heads of MHA\n",
    "        L: Number of transformer encoder layers\n",
    "    \n",
    "    Returns:\n",
    "        CCT Model\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = layers.Input(input_shape)  # Now expects RGB images with 3 channels\n",
    "    \n",
    "    # Convolutional Tokenization Block\n",
    "    conv_tokenizer = ConvolutionalTokenizer()\n",
    "    embedded_patches = conv_tokenizer(inputs)\n",
    "\n",
    "    # Adding positional embedding\n",
    "    pos_embed, seq_length = conv_tokenizer.pos_embeddings(image_size)\n",
    "    positions = tf.range(start=0, limit=seq_length, delta=1)\n",
    "    position_embeddings = pos_embed(positions)\n",
    "    embedded_patches += position_embeddings\n",
    "    \n",
    "    # Transformer Encoder Block with Sequence Pooling\n",
    "    embedded_patches = Transformer_Encoder(L, embedded_patches, num_heads=num_heads,\n",
    "                                           projection_dim=projection_dim, transformer_units=transformer_units)\n",
    "    \n",
    "    # Sequence Pooling on the output of the transformer encoder\n",
    "    sequence_pooling = SeqPool(embedded_patches)\n",
    "    \n",
    "    # Dense layer for classification\n",
    "    output = layers.Dense(num_classes, activation='softmax')(sequence_pooling)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " convolutional_tokenizer (Convo  (None, 3136, 128)   75456       ['input_1[0][0]']                \n",
      " lutionalTokenizer)                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 3136, 128)   0           ['convolutional_tokenizer[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 3136, 128)   256         ['tf.__operators__.add[0][0]']   \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 3136, 128)   131968      ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 3136, 128)    0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 3136, 128)   256         ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3136, 128)    16512       ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 3136, 128)    0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3136, 128)    16512       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 3136, 128)    0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 3136, 128)    0           ['dropout_1[0][0]',              \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 3136, 128)   256         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3136, 1)      129         ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " tf.nn.softmax (TFOpLambda)     (None, 3136, 1)      0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf.linalg.matmul (TFOpLambda)  (None, 1, 128)       0           ['tf.nn.softmax[0][0]',          \n",
      "                                                                  'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze (TFOpLamb  (None, 128)         0           ['tf.linalg.matmul[0][0]']       \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            258         ['tf.compat.v1.squeeze[0][0]']   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 241,603\n",
      "Trainable params: 241,603\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cct = CompactConvolutionalTransformer()\n",
    "print(cct.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 model weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "weights_path = r'E:\\Projects\\Content-moderation\\weights\\CCTmodel (1).hdf5'\n",
    "cct.load_weights(weights_path)\n",
    "\n",
    "\n",
    "print(\"HDF5 model weights loaded successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf211",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
